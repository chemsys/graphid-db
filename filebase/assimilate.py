#!/usr/bin/env python
import argparse
import hashlib
import json
import os
import re
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Tuple, Any

import pandas as pd
from pymatgen.core import Structure


def compute_file_hash(file_path: str) -> str:
    """Compute SHA-256 hash of a file."""
    sha256_hash = hashlib.sha256()
    with open(file_path, "rb") as f:
        for byte_block in iter(lambda: f.read(4096), b""):
            sha256_hash.update(byte_block)
    return sha256_hash.hexdigest()


def extract_source_info(file_path: str) -> Tuple[str, str, str]:
    """
    Extract source information from file path.
    Returns (source, proprietary_id, url) tuple.
    """
    # Extract source from directory structure
    path_parts = Path(file_path).parts
    source = "unknown"
    proprietary_id = ""
    url = ""

    # Try to determine source from path
    if "iza" in str(file_path).lower():
        source = "iza"
        # Extract IZA code if present
        match = re.search(r'([A-Z]{3,4})\.cif', file_path)
        if match:
            proprietary_id = match.group(1)
            url = f"http://www.iza-structure.org/databases/{proprietary_id}"
    elif "mp" in str(file_path).lower() or "materials_project" in str(file_path).lower():
        source = "mp"
        # Extract MP ID if present
        match = re.search(r'(mp-\d+)', file_path)
        if match:
            proprietary_id = match.group(1)
            url = f"https://materialsproject.org/materials/{proprietary_id}"

    return source, proprietary_id, url


def get_composition_hash(structure: Structure) -> str:
    """Get the first two characters of the composition hash."""
    comp = structure.composition
    comp_str = str(comp)
    comp_hash = hashlib.sha256(comp_str.encode()).hexdigest()
    return comp_hash[:2]


def assimilate_json_files(json_dir: str, output_dir: str) -> None:
    """
    Assimilate JSON files generated by compute.py and construct the file database.
    
    Args:
        json_dir: Directory containing JSON files
        output_dir: Directory to write the database
    """
    json_dir_path = Path(json_dir)
    output_dir_path = Path(output_dir)

    # Find all JSON files
    json_files = list(json_dir_path.glob("**/*.cif.json"))
    print(f"Found {len(json_files)} JSON files")

    # Dictionary to store entries by composition hash
    entries_by_comp_hash: Dict[str, Dict[str, Any]] = defaultdict(dict)

    # Process each JSON file
    for json_file in json_files:
        cif_file = json_file.with_suffix("")  # Remove .json extension
        if not cif_file.exists():
            print(f"Warning: CIF file {cif_file} not found, skipping")
            continue

        # Load structure and compute composition hash
        try:
            structure = Structure.from_file(str(cif_file))
            comp_hash = get_composition_hash(structure)
        except Exception as e:
            print(f"Error loading structure from {cif_file}: {e}")
            continue

        # Compute file hash
        file_hash = compute_file_hash(str(cif_file))
        file_hash_prefix = file_hash[:2]

        # Extract source information
        source, proprietary_id, url = extract_source_info(str(cif_file))

        # Load graph IDs from JSON
        try:
            with open(json_file, "r") as f:
                graph_ids = json.load(f)
        except Exception as e:
            print(f"Error loading JSON from {json_file}: {e}")
            continue

        # Create entry for each graph ID
        for graph_id_type, graph_id_value in graph_ids.items():
            if not graph_id_value:  # Skip empty graph IDs
                continue

            # Create entry
            entry = {
                "proprietary_id": proprietary_id,
                "datasource": source,
                "url": url,
                "filehash": file_hash
            }

            # Add to entries dictionary
            entries_by_comp_hash[comp_hash][graph_id_value] = entry

    # Create output directory structure and write entries.json files
    for comp_hash, entries in entries_by_comp_hash.items():
        # Create directory: composition_hash/filehash_prefix/entries.json
        for filehash_prefix in set(entry["filehash"][:2] for entry in entries.values()):
            dir_path = output_dir_path / comp_hash / filehash_prefix
            dir_path.mkdir(parents=True, exist_ok=True)

            # Filter entries for this filehash prefix
            filtered_entries = {
                graph_id: entry for graph_id, entry in entries.items()
                if entry["filehash"][:2] == filehash_prefix
            }

            # Write entries.json
            entries_file = dir_path / "entries.json"
            with open(entries_file, "w") as f:
                json.dump(filtered_entries, f, indent=2)

            print(f"Wrote {len(filtered_entries)} entries to {entries_file}")


def main():
    parser = argparse.ArgumentParser(description="Assimilate JSON files and construct file database")
    parser.add_argument("--json-dir", type=str, required=True, help="Directory containing JSON files")
    parser.add_argument("--output-dir", type=str, required=True, help="Directory to write the database")
    args = parser.parse_args()

    assimilate_json_files(args.json_dir, args.output_dir)


if __name__ == "__main__":
    main()
